+++
author = "Rachel Mead & Garrett Dash Nelson"
backgroundImage = ""
date = 2021-01-13T15:58:37Z
description = ""
draft = true
image = "https://twitter.com/auto_mapic/status/1317201921925677056/photo/1"
tags = []
title = "Automapic"

+++
Last fall, we launched a new experiment with the goal of discovering new materials in our collections: a bot that randomly pulls images from our digital collections and posts excerpts of them to Twitter. We call it [@auto_mapic](https://twitter.com/auto_mapic). Online search systems tend to turn up the results that match what you're looking for. But what if you don't know what you're looking for yet? Traditional digital collections repositories are much less effective for the kind of serendipity that can come from browsing a bookshelf or idly flipping through the pages of a catalogue for inspiration. That's why this random bot has been so intriguingâ€”it turns up things that we didn't know to look for. And, at a time when our physical spaces our closed, itâ€™s been a great way to engage with our collections from a safe distance.

At the same time, however, the bot also sparked a lot of discussion about our objects and how to present them in a way that takes responsibility for their content. We started this conversation when we set up the bot, by making its [very first post](https://twitter.com/auto_mapic/status/1301547799062945794) a message about how to contact us if its random selection happened to pull up an image with objectionable content. This concern became  more real a month later when auto_mapic tweeted this image from a 1949 puzzle map:

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Excerpt from "United States inlay puzzle" ([1949]) <a href="https://t.co/nlGkyU0bIX">https://t.co/nlGkyU0bIX</a> <a href="https://t.co/sx7n3iTl3J">pic.twitter.com/sx7n3iTl3J</a></p>â€” A U T O M A P I C ðŸ¤– (@auto_mapic) <a href="https://twitter.com/auto_mapic/status/1314197117007802368?ref_src=twsrc%5Etfw">October 8, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

Having this puzzle up on our botâ€™s feed without context didn't feel right, and we also feel strongly that we shouldn't hide behind the argument that "the algorithm chose it, not us." Such claims that hide responsibility behind a guise of technological determinism have been one of the most sinister ways of laundering prejudice and bias and making them seem natural.

At the same time, we didn't want to simply remove the image, since, after all, it's part of our physical and digital collections in the first place. Instead, we added the following text as a threaded reply:

This jigsaw puzzle, like many mass-consumption items with art produced by commercial illustrators, includes crude stereotypes of the United States's regional cultures, with racist depictions of Black Americans in the South. The crop randomly selected by the bot includes one of the most offensive of these images. The Delta region was and is home to some of the most original and important cultural traditions in the United States, including the famous Delta Blues. In this map, however, these are ignored in favor of an offensive cartoon. An image like this, appearing in a puzzle that would have been used by children and families, is a reminder of how racist stereotypes are reiterated through countless mundane examples of marginalization, mockery, and caricature.

This episode also opened up a conversation within our staff about how to handle the ambiguous importance of images like these. After all, having an object present in an important collection like ours (if we do say so ourselves!) lends it a sense of cultural significance and power. What kind of a message does it send when an image like this one is granted this kind of institutional cachet?

This puzzle is hardly the only object in our collection that contains racist, sexist, imperialist, or nationalistic imagery. Many pictorial maps from the twentieth century have this problem, with caricatures of ethnic groups. Quite a few of our older maps have depictions of cannibals around the margins of the New World, illustrating what was incorrectly assumed to be a primitive form of civilization. While these images are offensiveâ€”and, importantly, factually incorrectâ€”they also document a cultural history that cannot be contained and hidden. Instead of pretending like these cruel attitudes never existed, we think it's important to realize how maps and images were used to create edifices of unjust social and cultural power.

Should these maps be digitized and publicly available? What purpose do they serve the public? Do we have a responsibility to contextualize them, or should we trust that others can interpret for themselves? Is there a difference between someone finding this map in our collections and seeing it on Twitter? As a staff, we donâ€™t agree on the answers to all of these questions. Probably no group of people would! But we also have to recognize the limitations of our own perspectives, and reach out to understand how our collections are being seen by others.

One thing we've agreed on is to add a [Contextualizing Collections](https://www.leventhalmap.org/collections/contextualizing-collections/) page to our new website, describing some of our principles around the representational inequality in the materials that we steward. We're also working collectively with BPL staff to set guidance for how to handle historic collections online; even more than maps, photographs and illustrations are rife with these kinds of issues.

What's important to recognize, though, is that the legacies of these historic materials aren't confined exclusively to the past. The same attitudes that led this puzzle illustrator to label Louisiana with an offensive caricature in 1949 are, unfortunately, still attitudes that we struggle to combat throughout our society. Our hope is that by confronting the pastâ€”with an unflinching eye towards the parts that are often unpleasantâ€”we can equip ourselves with the kinds of understanding that can build a more respectful and tolerant future.